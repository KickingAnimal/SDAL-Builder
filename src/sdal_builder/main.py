#!/usr/bin/env python3
"""
CLI for building SDAL ISO image from OSM data.

FINAL PRODUCTION VERSION (this file):
- INIT.SDL (OEM mode) is now fully generated by code:
  * HEADER_T + COUNTRY_REF_T[] + FEATURE_SET_T[] + COUNTRY_INFO_T[]
  * CRC32 is recalculated over the fixed 0x12048-byte block.
  * Country table is built strictly from actually generated region *.SDL files.
  * Country Info immediately binds a voice language (ENG/UKE/DUT/...) + ENGF/ENGM etc.
- Parcel size limit enforcement (64KB max) via chunking (ALL large PIDs: Carto, Routing, POI, NAV, KD-TREE).
- Regional Cartography/B-Tree chunk size reduced for extreme safety to avoid repeated PID 2000 errors.
- All header and metadata encoding issues fixed.
"""
import sys
import time

# ============================================================================
# UX: INSTANT FEEDBACK
# ============================================================================
sys.stderr.write("[INFO] Initializing geospatial engine (loading GeoPandas/Shapely)... ")
sys.stderr.flush()
_t_start = time.time()

import argparse
import logging
import pathlib
import warnings
import shutil
from logging.handlers import RotatingFileHandler
from dataclasses import dataclass
from typing import Callable, Any, List, Tuple

import numpy as np
import geopandas as gpd
from shapely.geometry import LineString, MultiLineString, box, Point
from tqdm import tqdm
import struct
import zlib
import os
import math

sys.stderr.write(f"Done in {time.time() - _t_start:.1f}s\n")
sys.stderr.flush()

# ============================================================================
# IMPORTS
# ============================================================================

from .constants import (
    PIDS_OEM, PIDS_STD,
    MAX_USHORT, NO_COMPRESSION, UNCOMPRESSED_FLAG,
    PSF_VERSION_MAJOR, PSF_VERSION_MINOR, PSF_VERSION_YEAR,
    MARKER_TABLE, CONTINENT_MAP, HUFFMAN_TABLE
)

try:
    from .init_constants import OEM_INIT_HEADER  # now unused, kept for reference
except ImportError:
    OEM_INIT_HEADER = None

from .etl import (
    download_region_if_needed,
    region_exists,
    load_road_network,
    load_poi_data,
)
from .encoder import (
    encode_bytes,
    encode_strings,
    encode_cartography,
    encode_btree,
    encode_poi_index
)
from .iso import build_iso
from .translations import countries
from .routing_format import NodeRecord, SegmentRecord, deg_to_ntu, encode_routing_parcel
from .spatial import build_kdtree, serialize_kdtree

log = logging.getLogger(__name__)

# ============================================================================
# CONSTANTS & HELPERS (CRITICAL FIXES)
# ============================================================================

# CRITICAL FIX: SDAL parcel header size (uint16) max = 65535.
MAX_PARCEL_PAYLOAD = 65000


@dataclass
class TopologyEntry:
    db_id: int
    sdl_name: str
    parcel_id: int
    offset_units: int
    rect_min_lat_ntu: int
    rect_max_lat_ntu: int
    rect_min_lon_ntu: int
    rect_max_lon_ntu: int
    scale_min: int
    scale_max: int
    layer_type: int


@dataclass
class ParcelBuilder:
    pid: int
    layer_type: int
    make: Callable[[int], bytes]
    rect: Tuple[int, int, int, int]
    scale_min: int
    scale_max: int


def safe_encode_parcel(pid, payload, offset_units, compress_type=NO_COMPRESSION) -> bytes:
    """
    Encodes a parcel and HARD-FAILS if payload exceeds SDAL 64K limit.
    Prevents silent corruption on target head unit.
    """
    if len(payload) > MAX_PARCEL_PAYLOAD:
        raise ValueError(
            f"CRITICAL ERROR: Parcel PID {pid} payload size {len(payload)} "
            f"exceeds SDAL limit {MAX_PARCEL_PAYLOAD} bytes! Must chunk data."
        )
    return encode_bytes(pid, payload, offset_units=offset_units, compress_type=compress_type)

# ============================================================================
# HEADER ENCODERS (STANDARD 0.SDL / SDAL MODE)
# ============================================================================


def encode_glb_media_header(pids, sdl_files, regions, supp_langs, offset_locale, offset_compress) -> bytes:
    header = bytearray()
    header.extend(struct.pack(">HHHHHBB",
                              PSF_VERSION_MAJOR, PSF_VERSION_MINOR, PSF_VERSION_YEAR,
                              0, 0, 0, 0))
    header.extend(struct.pack(">HH", 0xFFFF, len(regions)))
    header.extend(struct.pack(">I", 0))

    parcel_sizes = bytearray(256)
    for pid in [pids.GLB_MEDIA_HEADER, pids.LOCALE, pids.SYMBOL, pids.CARTO,
                pids.BTREE, pids.ROUTING, pids.DENS, pids.POI_NAME,
                pids.POI_GEOM, pids.POI_INDEX, pids.KDTREE, pids.NAV]:
        if 0 <= pid < 256:
            parcel_sizes[pid] = 0
    header.extend(parcel_sizes)

    header.extend(struct.pack(">HHHHHH", 0, 0, 0, 0, 0, 0))
    header.extend(struct.pack(">HH", offset_locale, 0xFFFF))
    header.extend(struct.pack(">HH", offset_compress, 1))
    header.extend(struct.pack(">I", 0))

    if len(header) < 512:
        header.extend(b'\x00' * (512 - len(header)))
    return bytes(header)


def encode_locale_table(countries_dict, supported_langs) -> bytes:
    buf = bytearray()
    all_countries = sorted(countries_dict.keys())
    buf += struct.pack(">II", len(all_countries), len(supported_langs) + 1)

    lang_codes = [b"NATIVE"] + [lang.encode('ascii') for lang in supported_langs]
    for code in lang_codes:
        buf += code[:8].ljust(8, b'\x00')

    for country in all_countries:
        row = [country]
        t = countries_dict.get(country, {})
        for lang in supported_langs:
            row.append(t.get(lang, t.get("UKE", country)))
        for name in row:
            buf += name.encode('ascii', 'replace')[:32].ljust(32, b'\x00')
    return bytes(buf)


def encode_symbol_table(huffman) -> bytes:
    buf = bytearray()
    for i in range(256):
        buf += struct.pack(">BH", i, 0)
    buf += "".join(
        [chr(i) if 32 <= i < 127 else f"\\x{i:02x}"
         for i in range(256)]
    ).encode('ascii', 'replace')
    return bytes(buf)


def _encode_region_header(db_id, pids) -> bytes:
    header = bytearray()
    header.extend(struct.pack(">IIII", db_id, 0, 0, 0))
    header.extend(struct.pack(">HHHH", 1, 7, 1999, 0))

    layer_pcl_desc = bytearray(256)
    if pids.CARTO < 256:
        layer_pcl_desc[pids.CARTO] = 0
    if pids.ROUTING < 256:
        layer_pcl_desc[pids.ROUTING] = 0
    header.extend(layer_pcl_desc)

    if len(header) < 512:
        header.extend(b'\x00' * (512 - len(header)))
    return bytes(header)


def build_region_sdl_file(mode, out_path, db_id, sdl_name,
                          parcel_builders, topology_entries, pids):
    unit_shift = 12
    unit_size = 1 << unit_shift
    offset_bytes = 0

    with open(out_path, "wb") as f:
        if mode.upper() == "SDAL":
            rh = _encode_region_header(db_id, pids)
            f.write(rh)
            pad = (-len(rh)) & (unit_size - 1)
            if pad:
                f.write(b'\x00' * pad)
            offset_bytes += len(rh) + pad

        for pb in parcel_builders:
            offset_units = offset_bytes >> unit_shift
            parcel_bytes = pb.make(offset_units)

            parcel_with_header = safe_encode_parcel(pb.pid, parcel_bytes, offset_units)
            f.write(parcel_with_header)

            pad = (-len(parcel_with_header)) & (unit_size - 1)
            if pad:
                f.write(b"\x00" * pad)
            offset_bytes += len(parcel_with_header) + pad

            topology_entries.append(
                TopologyEntry(
                    db_id=db_id,
                    sdl_name=sdl_name,
                    parcel_id=pb.pid,
                    offset_units=offset_units,
                    rect_min_lat_ntu=pb.rect[0],
                    rect_max_lat_ntu=pb.rect[1],
                    rect_min_lon_ntu=pb.rect[2],
                    rect_max_lon_ntu=pb.rect[3],
                    scale_min=pb.scale_min,
                    scale_max=pb.scale_max,
                    layer_type=pb.layer_type,
                )
            )

# ============================================================================
# INIT.SDL WRITERS
# ============================================================================


def write_init_sdl_standard(dst_path, sdl_files, regions, supp_lang, pids):
    """
    SDAL-mode INIT (0.SDL replacement) – our original synthetic global media file.
    Used only when format_mode == "SDAL".
    """
    unit_shift = 12
    unit_size = 1 << unit_shift
    offset_bytes = 0

    supp_langs = [s.strip().upper() for s in supp_lang.split(',')] if supp_lang else ["UKE"]

    payload_locale = encode_locale_table(countries, supp_langs)
    payload_symbol = encode_symbol_table(HUFFMAN_TABLE)

    ph_size = 532
    pad1 = (-ph_size) & (unit_size - 1)
    off_locale = (ph_size + pad1) // unit_size
    loc_size = len(payload_locale)
    off_comp = off_locale + ((loc_size + 20) // unit_size)

    payload_header = encode_glb_media_header(pids, sdl_files, regions, supp_langs, off_locale, off_comp)

    with open(dst_path, "wb") as f:
        # GLB_MEDIA_HEADER
        ph = safe_encode_parcel(pids.GLB_MEDIA_HEADER, payload_header, 0)
        f.write(ph)
        pad = (-len(ph)) & (unit_size - 1)
        if pad:
            f.write(b"\x00" * pad)
        offset_bytes += len(ph) + pad

        # LOCALE
        pl = safe_encode_parcel(pids.LOCALE, payload_locale, offset_bytes >> unit_shift)
        f.write(pl)
        pad = (-len(pl)) & (unit_size - 1)
        if pad:
            f.write(b"\x00" * pad)
        offset_bytes += len(pl) + pad

        # SYMBOL TABLE
        ps = safe_encode_parcel(pids.SYMBOL, payload_symbol, offset_bytes >> unit_shift)
        f.write(ps)
        pad = (-len(ps)) & (unit_size - 1)
        if pad:
            f.write(b"\x00" * pad)


def write_oem_init_sdl(
    dst_path: pathlib.Path,
    generated_files: list[pathlib.Path],
    regions: list[str],
    supp_lang
) -> None:
    """
    OEM-mode INIT.SDL generator.

    Fully replaces OEM_INIT_HEADER-based hack. We now generate a fixed-size
    0x12048-byte configuration block with the following layout:

      [0x0000..0x00FF]  HEADER_T (256 bytes)
      [offset_country_refs ..]  COUNTRY_REF_T[N] (32 bytes each)
      [offset_feature_sets ..]  FEATURE_SET_T[N] (20 bytes each)
      [offset_country_infos ..] COUNTRY_INFO_T[N] (32 bytes each)
      [rest] zero padding

    HEADER_T (little endian):
      0x00  uint32  magic            (example: 0x4C414453 = 'SDAL')
      0x04  uint32  format_version   (e.g. 0x00010001)
      0x08  uint32  total_payload_size (always 0x12048)
      0x0C  uint32  crc32            (IEEE CRC32, init=0xFFFFFFFF, xor=0xFFFFFFFF)
      0x10  uint32  country_count    (N)
      0x14  uint32  offset_country_refs (absolute from 0x0000)
      0x18  uint32  offset_feature_sets (absolute from 0x0000)
      0x1C  12 bytes reserved/padding (zeros)
      0x28..0xFF reserved (zeros)

    COUNTRY_REF_T (32 bytes, little endian):
      0x00  uint16  country_id
      0x02  char[5] country_code_str (e.g. 'CYPRU', 'BENEL')
      0x07  uint8   chain_flags      (0 for now)
      0x08  uint32  affix_offset     (-> COUNTRY_INFO_T for this country)
      0x0C  uint32  feature_offset   (-> FEATURE_SET_T for this country)
      0x10  uint16  media_profile_id (0 for now)
      0x12  uint16  checksum_short   (simple local checksum)
      0x14  uint8[12] padding

    FEATURE_SET_T (20 bytes, little endian):
      0x00  uint16  default_region_code  (we use country_id)
      0x02  uint16  default_language_code (internal language index)
      0x04  uint32  feature_bitmask_1    (basic navigation flag)
      0x08  uint32  feature_bitmask_2
      0x0C  uint32  feature_bitmask_3
      0x10  uint32  model_trim_id

    COUNTRY_INFO_T (32 bytes, little endian – simplified GlbCountry_t analogue):
      0x00  uint16  country_id
      0x02  uint16  phone_country_code (E.164, 0 if unknown)
      0x04  uint8   national_lang_code (same index as default_language_code)
      0x05  uint8   driving_side       (0=right, 1=left)
      0x06  uint8   measurement_system (0=metric, 1=imperial)
      0x07  uint8   currency_type      (0=unknown, otherwise OEM enum)
      0x08  char[3] voice_lang         (e.g. 'ENG', 'UKE', 'DUT')
      0x0B  char[4] voice_female_tag   (e.g. 'ENGF')
      0x0F  char[4] voice_male_tag     (e.g. 'ENGM')
      0x13  uint8[13] reserved

    IMPORTANT BEHAVIOUR:
    - We DO NOT touch or parse voice .SDL files themselves – they are just present on disc.
    - Country table is built strictly from regions that have a corresponding {STEM}1.SDL
      in `generated_files`, where STEM is last path component of region slug.
    - Voice language for a country is decided heuristically from its name
      (e.g. CYPRUS → ENG, UNITED KINGDOM → UKE). When no specific mapping
      is known, we fall back to ENG/ENGF/ENGM.
    """

    log = logging.getLogger(__name__)

    BLOCK_SIZE = 0x12048
    HEADER_SIZE = 0x100
    COUNTRY_REF_SIZE = 32
    FEATURE_SET_SIZE = 20
    COUNTRY_INFO_SIZE = 32

    # Build quick lookup of actually generated SDL filenames (upper-case).
    generated_names = {p.name.upper() for p in generated_files}

    # ---- Language + Voice mapping tables ----------------------------------
    voice_lang_codes = ["ENG", "UKE", "DUT", "FRE", "GER", "ITA", "JPN", "SPA"]
    lang_index_map = {code: idx + 1 for idx, code in enumerate(voice_lang_codes)}

    # Language → voice F/M SDL filenames (only names, not touched on disk).
    voice_files = {
        "DUT": ("DUTF.SDL", "DUTM.SDL"),
        "ENG": ("ENGF.SDL", "ENGM.SDL"),
        "FRE": ("FREF.SDL", "FREM.SDL"),
        "GER": ("GERF.SDL", "GERM.SDL"),
        "ITA": ("ITAF.SDL", "ITAM.SDL"),
        "JPN": ("JPNF.SDL", "JPNM.SDL"),
        "SPA": ("SPAF.SDL", "SPAM.SDL"),
        "UKE": ("UKEF.SDL", "UKEM.SDL"),
    }

    # Heuristic mapping Country → language (3-letter code).
    voice_lang_by_country = {
        "CYPRUS": "ENG",
        "BENELUX": "DUT",
        "NETHERLANDS": "DUT",
        "BELGIUM": "FRE",
        "LUXEMBOURG": "FRE",
        "GERMANY": "GER",
        "FRANCE": "FRE",
        "ITALY": "ITA",
        "SPAIN": "SPA",
        "PORTUGAL": "SPA",
        "UNITED KINGDOM": "UKE",
        "GREAT BRITAIN": "UKE",
        "UK": "UKE",
        "IRELAND": "ENG",
    }

    # Left-hand driving countries – minimal EU-focused set.
    left_hand_countries = {"UNITED KINGDOM", "CYPRUS", "IRELAND", "MALTA", "JAPAN"}

    # Phone country codes (E.164). 0 if unknown.
    phone_codes = {
        "CYPRUS": 357,
        "GERMANY": 49,
        "FRANCE": 33,
        "ITALY": 39,
        "SPAIN": 34,
        "PORTUGAL": 351,
        "UNITED KINGDOM": 44,
        "IRELAND": 353,
        "NETHERLANDS": 31,
        "BELGIUM": 32,
        "LUXEMBOURG": 352,
    }

    def pick_voice_lang(country_name: str) -> str:
        """
        Choose voice language code for a given country.
        CYPRUS explicitly falls back to ENG (ENGF/ENGM) because no Greek voices exist.
        """
        key = country_name.upper()
        if key in voice_lang_by_country:
            return voice_lang_by_country[key]
        if "UNITED" in key and "KINGDOM" in key:
            return "UKE"
        return "ENG"

    # ---- Build country entries strictly from regions that have {STEM}1.SDL ----
    country_entries = []
    next_country_id = 1

    for region_slug in regions:
        # STEM is last component of slug; region SDLs are STEM0.SDL and STEM1.SDL.
        stem = pathlib.Path(region_slug).name.upper().replace("-", "_")
        map_name = f"{stem}1.SDL"

        if map_name.upper() not in generated_names:
            log.warning(
                "Skipping region %s in INIT: map SDL %s not found among generated files",
                region_slug, map_name
            )
            continue

        country_name = extract_country(region_slug)  # already UPPER and spaced
        # Country code string: first 5 non-space characters (BELGI, CYPRU, etc.)
        code5_str = country_name.replace(" ", "")[:5].ljust(5)
        voice_lang = pick_voice_lang(country_name)

        female_file, male_file = voice_files.get(voice_lang, voice_files["ENG"])

        entry = {
            "id": next_country_id,
            "name": country_name,
            "code5": code5_str.encode("ascii", "replace"),
            "voice_lang": voice_lang,
            "voice_female_tag": female_file.split(".")[0].encode("ascii", "replace"),
            "voice_male_tag": male_file.split(".")[0].encode("ascii", "replace"),
        }
        country_entries.append(entry)
        next_country_id += 1

    # If for some reason nothing survived – still emit a syntactically valid block.
    if not country_entries:
        buf = bytearray(BLOCK_SIZE)
        magic = 0x4C414453  # 'SDAL'
        version = 0x00010000
        total_size = BLOCK_SIZE
        struct.pack_into("<III", buf, 0x00, magic, version, total_size)
        crc = zlib.crc32(buf[0x10:], 0xFFFFFFFF) ^ 0xFFFFFFFF
        struct.pack_into("<I", buf, 0x0C, crc & 0xFFFFFFFF)
        dst_path.write_bytes(bytes(buf))
        return

    country_count = len(country_entries)
    offset_country_refs = HEADER_SIZE
    offset_feature_sets = offset_country_refs + country_count * COUNTRY_REF_SIZE
    offset_country_infos = offset_feature_sets + country_count * FEATURE_SET_SIZE

    if offset_country_infos + country_count * COUNTRY_INFO_SIZE > BLOCK_SIZE:
        raise ValueError("INIT.SDL OEM layout does not fit fixed 0x12048-byte block")

    # ---- Assemble full 0x12048-byte block in memory ---------------------------
    buf = bytearray(BLOCK_SIZE)

    magic = 0x4C414453  # 'SDAL'
    version = 0x00010001
    total_size = BLOCK_SIZE
    crc_placeholder = 0

    # HEADER_T
    struct.pack_into(
        "<IIIIIII12s",
        buf,
        0x00,
        magic,
        version,
        total_size,
        crc_placeholder,
        country_count,
        offset_country_refs,
        offset_feature_sets,
        b"\x00" * 12,
    )

    # COUNTRY_REF_T, FEATURE_SET_T and COUNTRY_INFO_T struct codecs
    country_ref_struct = struct.Struct("<H5sBIIHH12s")
    feature_struct = struct.Struct("<HHIIII")
    country_info_struct = struct.Struct("<HHBBBB3s4s4s13s")

    # Fill arrays in parallel: REF → FEATURE → COUNTRY_INFO
    for idx, entry in enumerate(country_entries):
        cid = entry["id"]
        feature_offset = offset_feature_sets + idx * FEATURE_SET_SIZE
        info_offset = offset_country_infos + idx * COUNTRY_INFO_SIZE

        lang_index = lang_index_map.get(entry["voice_lang"], 0)

        # --- FEATURE_SET_T ----------------------------------------------------
        feature_struct.pack_into(
            buf,
            feature_offset,
            cid,             # default region code (we use country id)
            lang_index,      # default language code
            0x00000001,      # feature bitmask 1: basic navigation enabled
            0,               # feature bitmask 2
            0,               # feature bitmask 3
            0,               # model/trim ID
        )

        # --- COUNTRY_INFO_T (simplified GlbCountry_t analogue) ---------------
        name_key = entry["name"].upper()
        phone_code = phone_codes.get(name_key, 0)
        driving_side = 1 if name_key in left_hand_countries else 0
        meas_system = 0  # metric for Europe
        currency_type = 0  # unknown / default

        voice_lang_bytes = entry["voice_lang"].encode("ascii", "replace")[:3].ljust(3, b"\x00")
        female_tag = entry["voice_female_tag"][:4].ljust(4, b"\x00")
        male_tag = entry["voice_male_tag"][:4].ljust(4, b"\x00")

        country_info_struct.pack_into(
            buf,
            info_offset,
            cid,
            phone_code,
            lang_index,
            driving_side,
            meas_system,
            currency_type,
            voice_lang_bytes,
            female_tag,
            male_tag,
            b"\x00" * 13,
        )

        # --- COUNTRY_REF_T ---------------------------------------------------
        local_checksum = (cid + phone_code + feature_offset + info_offset) & 0xFFFF

        country_ref_struct.pack_into(
            buf,
            offset_country_refs + idx * COUNTRY_REF_SIZE,
            cid,
            entry["code5"],
            0,                # chain_flags
            info_offset,      # affix_offset -> COUNTRY_INFO_T
            feature_offset,   # feature_offset -> FEATURE_SET_T
            0,                # media_profile_id
            local_checksum,
            b"\x00" * 12,
        )

    # ---- CRC32 (critical) ----------------------------------------------------
    # IEEE 802.3 CRC32, poly 0x04C11DB7, init 0xFFFFFFFF, final XOR 0xFFFFFFFF
    crc = zlib.crc32(buf[0x10:], 0xFFFFFFFF) ^ 0xFFFFFFFF
    struct.pack_into("<I", buf, 0x0C, crc & 0xFFFFFFFF)

    # Write final INIT.SDL
    dst_path.write_bytes(bytes(buf))

# ============================================================================
# METADATA HELPERS
# ============================================================================


def marker_for_file(name: str) -> bytes:
    name = name.upper()
    if name.endswith("0.SDL") or name.endswith("1.SDL"):
        return MARKER_TABLE.get("MAP", MARKER_TABLE.get("OTHER"))
    base = name.split('.')[0]
    return MARKER_TABLE.get(base, MARKER_TABLE.get("OTHER"))


OEM_HEADER = b"SDAL" + b"\x00" * 12
REGION_LABEL_MAXLEN = 14
LANG_FIELD_MAXLEN = 30
REGION_TABLE_ENTRY_SIZE = 16


def extract_continent(region_slugs):
    if not region_slugs:
        return "UNKNOWN"
    return region_slugs[0].split('/')[0].upper()


def extract_disc_code(region_slugs):
    continent = extract_continent(region_slugs)
    code = CONTINENT_MAP.get(continent)
    if code:
        return code
    if len(continent) >= 2:
        return continent[:2]
    return "XX"


def extract_country(region_slug: str) -> str:
    """
    Convert 'europe/cyprus' → 'CYPRUS', 'europe/united-kingdom' → 'UNITED KINGDOM'.
    """
    return region_slug.split('/')[-1].replace('-', ' ').replace('_', ' ').upper()


def build_region_translation_table(region_slugs, supp_langs, countries_dict):
    table = []
    for slug in region_slugs:
        native = extract_country(slug)
        row = [native]
        trans_map = countries_dict.get(native, {})
        for lang in supp_langs:
            row.append(trans_map.get(lang, trans_map.get("UKE", native)))
        table.append(row)
    return table


def write_region_sdl(path, region_slugs, supp_lang, countries_dict):
    label = extract_continent(region_slugs)
    supp_langs = [s.strip().upper() for s in supp_lang.split(',')] if supp_lang else ["UKE"]

    table = build_region_translation_table(region_slugs, supp_langs, countries_dict)

    header = OEM_HEADER
    label_field = label.encode('ascii', 'replace')[:REGION_LABEL_MAXLEN].ljust(
        REGION_LABEL_MAXLEN, b' '
    ) + b'\x00'
    lang_field = b''.join(lang.encode('ascii', 'replace')[:3] for lang in supp_langs)
    lang_field = lang_field[:LANG_FIELD_MAXLEN].ljust(LANG_FIELD_MAXLEN, b' ') + b'\x00'

    region_table = b''
    for row in table:
        for name in row:
            region_table += name.encode('ascii', 'replace')[:REGION_TABLE_ENTRY_SIZE].ljust(
                REGION_TABLE_ENTRY_SIZE, b'\x00'
            )
        for _ in range(10 - len(row)):
            region_table += b'\x00' * REGION_TABLE_ENTRY_SIZE

    body = header + label_field + lang_field + region_table
    if len(body) < 4096:
        body += b'\x00' * (4096 - len(body))
    with open(path, "wb") as f:
        f.write(body)


def write_regions_sdl(path, region_slugs, supp_lang, countries_dict):
    supp_langs = [s.strip().upper() for s in supp_lang.split(',')] if supp_lang else ["UKE"]
    table = build_region_translation_table(region_slugs, supp_langs, countries_dict)

    header = OEM_HEADER
    region_table = b''
    for row in table:
        for name in row:
            region_table += name.encode('ascii', 'replace')[:REGION_TABLE_ENTRY_SIZE].ljust(
                REGION_TABLE_ENTRY_SIZE, b'\x00'
            )
        for _ in range(10 - len(row)):
            region_table += b'\x00' * REGION_TABLE_ENTRY_SIZE

    body = header + region_table
    if len(body) < 4096:
        body += b'\x00' * (4096 - len(body))
    with open(path, "wb") as f:
        f.write(body)


def write_mtoc_sdl(path, files):
    buf = bytearray(b"\x00" * 64)
    next_id = 1

    for fpath in files:
        name = fpath.name.upper()
        rec = bytearray(b"\x00" * 64)
        rec[8:8 + 16] = name.encode('ascii', 'replace')[:16].ljust(16, b'\x00')

        marker = marker_for_file(name)
        rec[28] = marker[0]
        rec[29:32] = struct.pack(">I", next_id)[1:]
        next_id += 1

        buf.extend(rec)

    if len(buf) < 4096:
        buf.extend(b"\x00" * (4096 - len(buf)))
    with open(path, "wb") as f:
        f.write(buf)


def write_cartotop_sdl(path: pathlib.Path,
                       entries: List[TopologyEntry],
                       pid_cartotop: int):
    """
    CARTOTOP.SDL – multi-parcel cartography topology, used by OEM to locate map
    parcels for each DB / region.
    """
    if not entries:
        payload = b"\x00" * 18
    else:
        min_lat = min(e.rect_min_lat_ntu for e in entries)
        max_lat = max(e.rect_max_lat_ntu for e in entries)
        min_lon = min(e.rect_min_lon_ntu for e in entries)
        max_lon = max(e.rect_max_lon_ntu for e in entries)

        buf = bytearray()
        buf += struct.pack(">iiii", min_lon, min_lat, max_lon, max_lat)
        buf += struct.pack(">H", len(entries))

        for e in entries:
            buf += struct.pack(">iiii",
                               e.rect_min_lon_ntu, e.rect_min_lat_ntu,
                               e.rect_max_lon_ntu, e.rect_max_lat_ntu)
            buf += struct.pack(">HHHHH",
                               e.db_id, e.parcel_id, e.layer_type,
                               e.scale_min, e.scale_max)
            buf += b"\x00\x00"
        payload = bytes(buf)

    # Wrap as SDAL parcel
    blob = encode_bytes(pid_cartotop, payload, offset_units=0)
    with open(path, "wb") as f:
        f.write(safe_encode_parcel(pid_cartotop, payload, 0))
        pad = (-len(blob)) & (4096 - 1)
        if pad:
            f.write(b"\x00" * pad)


def _encode_kdtree_idx_header(kd_data_len: int,
                              min_lat: int, max_lat: int,
                              min_lon: int, max_lon: int) -> bytes:
    """
    Encodes IDxPclHdr_t (32 bytes) for KDTREE.
    Uses signed integers for coordinates.
    """
    _IDXPCL_STRUCT = struct.Struct(">H H I I i i H H H H")
    return _IDXPCL_STRUCT.pack(
        1, 1,               # version, flags
        0, kd_data_len,     # reserved, data length
        min_lat, min_lon,   # origin coords
        0, 0, 0, 0          # reserved
    )

# ============================================================================
# LOGGING, FILE COPY, GEOMETRY HELPERS
# ============================================================================


def init_logging(verbose: bool, work_dir: pathlib.Path):
    work_dir.mkdir(parents=True, exist_ok=True)
    log_file = work_dir / "sdal_builder.log"

    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG if verbose else logging.INFO)

    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.DEBUG if verbose else logging.INFO)
    formatter = logging.Formatter("[%(asctime)s][%(levelname)s] %(message)s")
    ch.setFormatter(formatter)
    logger.addHandler(ch)

    fh = RotatingFileHandler(log_file, maxBytes=5_000_000, backupCount=3)
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(formatter)
    logger.addHandler(fh)


def copy_oem_sdl_files(work_dir: pathlib.Path) -> list[pathlib.Path]:
    """
    Copy OEM *.SDL files from project_root/oem_sdl into work_dir, except INIT.SDL.
    Returns list of copied paths.
    """
    result: list[pathlib.Path] = []
    try:
        project_root = pathlib.Path(__file__).resolve().parents[2]
        oem_dir = project_root / "oem_sdl"
        if not oem_dir.exists():
            return result

        for src in oem_dir.glob("*.SDL"):
            if "INIT" in src.name.upper():
                continue
            dst = work_dir / src.name
            if not dst.exists():
                shutil.copy2(src, dst)
            result.append(dst)
    except Exception:
        # Non-fatal – just work without OEM extras.
        pass
    return result


def _iter_coords(geom):
    if isinstance(geom, LineString):
        yield from geom.coords
    elif isinstance(geom, MultiLineString):
        for part in geom.geoms:
            yield from part.coords


def choose_scale_shift_for_nodes(nodes: List[NodeRecord]) -> int:
    """
    Simple scale heuristic; detailed scaling logic lives in routing_format.py.
    """
    return 12


def build_routing_graph_from_roads_df(roads_df) -> tuple[list[NodeRecord], list[SegmentRecord]]:
    nodes_map = {}
    node_records = {}
    segments: list[SegmentRecord] = []

    next_node_id = 0
    next_seg_id = 0

    for _, row in roads_df.iterrows():
        geom = row["geometry"]
        coords = list(_iter_coords(geom))
        if len(coords) < 2:
            continue

        line_node_ids = []
        for lon, lat in coords:
            key = (lon, lat)
            nid = nodes_map.get(key)
            if nid is None:
                nid = next_node_id
                nodes_map[key] = nid
                node_records[nid] = NodeRecord(
                    node_id=nid,
                    lat_deg=lat,
                    lon_deg=lon
                )
                next_node_id += 1
            line_node_ids.append(nid)

        for i in range(len(line_node_ids) - 1):
            n1 = line_node_ids[i]
            n2 = line_node_ids[i + 1]
            lon1, lat1 = coords[i]
            lon2, lat2 = coords[i + 1]

            mean_lat_rad = math.radians((lat1 + lat2) * 0.5)
            dx = (lon2 - lon1) * 111_320.0 * math.cos(mean_lat_rad)
            dy = (lat2 - lat1) * 110_540.0
            length_m = (dx * dx + dy * dy) ** 0.5

            seg = SegmentRecord(
                seg_id=next_seg_id,
                from_node_id=n1,
                to_node_id=n2,
                length_m=length_m,
                speed_class=0,
                oneway=0,
            )
            segments.append(seg)
            node_records[n1].segment_ids.append(seg.seg_id)
            node_records[n2].segment_ids.append(seg.seg_id)
            next_seg_id += 1

    ordered_nodes = [node_records[nid] for nid in sorted(node_records.keys())]
    return ordered_nodes, segments

# ============================================================================
# MAIN BUILD FUNCTION
# ============================================================================


def build(regions: list[str],
          out_iso: pathlib.Path,
          work: pathlib.Path,
          region_label=None,
          supp_lang=None,
          format_mode: str = "OEM"):
    log = logging.getLogger(__name__)

    if format_mode.upper() == "OEM":
        PIDS = PIDS_OEM
        log.info("Using OEM (Denso/Mazda) PID Profile")
    else:
        PIDS = PIDS_STD
        log.info("Using Standard SDAL 1.7 PID Profile")

    # Ensure OSM regions are present
    for region in regions:
        download_region_if_needed(region, work)

    topology_entries: list[TopologyEntry] = []
    db_for_region: dict[str, int] = {r: i + 1 for i, r in enumerate(regions)}

    # -------------------------------------------------------------------------
    # 1. POI processing – global (across all regions)
    # -------------------------------------------------------------------------
    all_poi_names: list[str] = []
    poi_records = []
    poi_offsets = []
    poi_coords: list[tuple[float, float]] = []
    offset_acc = 0
    poi_index_counter = 0

    for region in regions:
        pbf_path = work / f"{region.replace('/', '-')}.osm.pbf"
        pois_df = load_poi_data(pbf_path=str(pbf_path), logger=log, poi_tags=None)

        all_poi_names.extend(pois_df["name"].fillna("").tolist())

        for geom_idx, geom in zip(pois_df.index, pois_df.geometry):
            if not isinstance(geom, Point):
                geom = geom.centroid
            lon, lat = geom.x, geom.y

            payload = struct.pack("<ii", int(lat * 1e6), int(lon * 1e6))
            poi_records.append((int(poi_index_counter), payload))
            poi_offsets.append((int(poi_index_counter), offset_acc))

            offset_acc += len(payload) + 6
            poi_coords.append((lon, lat))
            poi_index_counter += 1

    # -------------------------------------------------------------------------
    # 2. GLOBAL KD-TREE (for POI)
    # -------------------------------------------------------------------------
    log.info("Building Global KD-Tree...")
    if not poi_coords:
        kd_data = b""
        min_lat_ntu = max_lat_ntu = min_lon_ntu = max_lon_ntu = 0
    else:
        kd_tree_obj = build_kdtree(poi_coords)
        kd_data = serialize_kdtree(kd_tree_obj)

        all_lons = [c[0] for c in poi_coords]
        all_lats = [c[1] for c in poi_coords]
        min_lat_ntu, min_lon_ntu = deg_to_ntu(min(all_lats), min(all_lons))
        max_lat_ntu, max_lon_ntu = deg_to_ntu(max(all_lats), max(all_lons))

        del kd_tree_obj

    global_files: list[pathlib.Path] = copy_oem_sdl_files(work)

    # -------------------------------------------------------------------------
    # 3. Global POI-related SDLs: POINAMES, POIGEOM, POI INDEX
    # -------------------------------------------------------------------------
    MAX_POI_NAME_STRINGS = 2000
    poi_name_file = work / "POINAMES.SDL"

    log.info("Chunking %d POI names into parcels (PID %d)...",
             len(all_poi_names), PIDS.POI_NAME)

    if not all_poi_names:
        poi_name_file.write_bytes(safe_encode_parcel(PIDS.POI_NAME, b'', 0))
    else:
        with open(poi_name_file, "wb") as f:
            for i in tqdm(range(0, len(all_poi_names), MAX_POI_NAME_STRINGS),
                          desc="POI Name Parcels"):
                name_chunk = all_poi_names[i:i + MAX_POI_NAME_STRINGS]
                raw_payload = encode_strings(PIDS.POI_NAME, name_chunk)
                parcel_with_header = safe_encode_parcel(PIDS.POI_NAME, raw_payload, 0)
                f.write(parcel_with_header)

    global_files.append(poi_name_file)

    poi_geom_file = work / "POIGEOM.SDL"
    MAX_POI_RECORDS_CHUNK = 5000

    log.info("Chunking %d POI geom/index records...", len(poi_records))

    with open(poi_geom_file, "wb") as f:
        # 1) POI GEOMETRY
        for i in tqdm(range(0, len(poi_records), MAX_POI_RECORDS_CHUNK),
                      desc="POI Geom Parcels"):
            geom_chunk = poi_records[i:i + MAX_POI_RECORDS_CHUNK]
            poi_geom_blob_chunk = b"".join(
                struct.pack(">H", UNCOMPRESSED_FLAG) + rec[1]
                for rec in geom_chunk
            )
            f.write(safe_encode_parcel(PIDS.POI_GEOM, poi_geom_blob_chunk, 0))

        # 2) POI INDEX
        for i in tqdm(range(0, len(poi_offsets), MAX_POI_RECORDS_CHUNK),
                      desc="POI Index Parcels"):
            index_chunk = poi_offsets[i:i + MAX_POI_RECORDS_CHUNK]
            poi_index_payload_chunk = encode_poi_index(PIDS.POI_INDEX, index_chunk)
            f.write(safe_encode_parcel(PIDS.POI_INDEX, poi_index_payload_chunk, 0))

    global_files.append(poi_geom_file)

    # -------------------------------------------------------------------------
    # 4. GLOBAL KD-TREE SDL (PID KDTREE) – chunked
    # -------------------------------------------------------------------------
    kd_file = work / "KDTREE.SDL"

    if not kd_data:
        kd_payload_empty = _encode_kdtree_idx_header(0, 0, 0, 0, 0)
        kd_file.write_bytes(safe_encode_parcel(PIDS.KDTREE, kd_payload_empty, 0))
    else:
        KDTREE_NODE_SIZE = 12
        KD_HEADER_SIZE = 32
        MAX_KDTREE_DATA_BYTES = MAX_PARCEL_PAYLOAD - KD_HEADER_SIZE
        MAX_KDTREE_NODES_PER_PARCEL = MAX_KDTREE_DATA_BYTES // KDTREE_NODE_SIZE

        log.info("Chunking %d KD-Tree nodes into parcels (PID %d)...",
                 len(kd_data) // KDTREE_NODE_SIZE, PIDS.KDTREE)

        chunk_size_bytes = MAX_KDTREE_NODES_PER_PARCEL * KDTREE_NODE_SIZE

        with open(kd_file, "wb") as f:
            is_first_chunk = True
            for i in tqdm(range(0, len(kd_data), chunk_size_bytes),
                          desc="KD-Tree Parcels"):
                data_chunk = kd_data[i:i + chunk_size_bytes]

                if is_first_chunk:
                    current_header = _encode_kdtree_idx_header(
                        len(data_chunk),
                        min_lat_ntu, max_lat_ntu,
                        min_lon_ntu, max_lon_ntu,
                    )
                    raw_payload = current_header + data_chunk
                    is_first_chunk = False
                else:
                    raw_payload = data_chunk

                f.write(safe_encode_parcel(PIDS.KDTREE, raw_payload, 0))

    global_files.append(kd_file)

    # -------------------------------------------------------------------------
    # 5. DENSITY (DENSxx0/1.SDL) – per-disc, derived from roads
    # -------------------------------------------------------------------------
    disc_code = extract_disc_code(regions)
    dens_tiles = []

    for region in regions:
        pbf_path = work / f"{region.replace('/', '-')}.osm.pbf"
        roads_df = load_road_network(str(pbf_path))
        log.info("Loaded %d road geometries for density from %s", len(roads_df), region)

        bbox = roads_df.total_bounds
        minx, miny, maxx, maxy = bbox

        center_x = (minx + maxx) / 2.0
        utm_zone = int((center_x + 180) / 6) + 1
        utm_crs = f"EPSG:{32600 + utm_zone}"

        roads_proj = roads_df.to_crs(utm_crs)
        proj_bounds = roads_proj.total_bounds
        pminx, pminy, pmaxx, pmaxy = proj_bounds

        roads_simple = roads_proj.explode(ignore_index=True)
        roads_simple = roads_simple[roads_simple.geometry.type.isin(
            ["LineString", "MultiLineString"]
        )]

        total_tiles_count = sum(4 ** z for z in range(4))
        log.info("Generating Density tiles for %s...", region)

        with tqdm(total=total_tiles_count, desc=f"Density {region}", unit="tile") as pbar:
            for Z in range(0, 4):
                num_tiles = 2 ** Z
                tile_width = (pmaxx - pminx) / num_tiles
                tile_height = (pmaxy - pminy) / num_tiles

                for tx in range(num_tiles):
                    for ty in range(num_tiles):
                        tminx = pminx + tx * tile_width
                        tmaxx = pminx + (tx + 1) * tile_width
                        tminy = pminy + ty * tile_height
                        tmaxy = pminy + (ty + 1) * tile_height

                        grid_size = 256
                        dx = (tmaxx - tminx) / grid_size
                        dy = (tmaxy - tminy) / grid_size

                        density_array = np.zeros((grid_size, grid_size), dtype=np.float64)
                        tile_box = box(tminx, tminy, tmaxx, tmaxy)
                        clipped = roads_simple.geometry.intersection(tile_box)

                        max_seg_length = min(dx, dy) / 2.0

                        for seg_geom in clipped:
                            if seg_geom.is_empty:
                                continue

                            if seg_geom.geom_type == "LineString":
                                geoms = [seg_geom]
                            elif seg_geom.geom_type == "MultiLineString":
                                geoms = list(seg_geom.geoms)
                            else:
                                geoms = []

                            for g in geoms:
                                total_len = g.length
                                if total_len <= 0:
                                    continue

                                n_pieces = max(1, int(np.ceil(total_len / max_seg_length)))
                                fractions = np.linspace(0, 1, n_pieces + 1)

                                prev_pt = seg_geom.interpolate(fractions[0], normalized=True)
                                for i in range(1, len(fractions)):
                                    curr_pt = seg_geom.interpolate(fractions[i], normalized=True)
                                    seg_len = prev_pt.distance(curr_pt)

                                    midpoint = LineString([prev_pt, curr_pt]).centroid
                                    mx, my = midpoint.x, midpoint.y

                                    col = int((mx - tminx) // dx)
                                    row = int((my - tminy) // dy)
                                    if 0 <= col < grid_size and 0 <= row < grid_size:
                                        density_array[row, col] += seg_len
                                    prev_pt = curr_pt

                        max_val = density_array.max()
                        scale = 65535.0 / max_val if max_val > 0 else 0.0
                        density_scaled = (
                            (density_array * scale)
                            .clip(0, 65535)
                            .astype(np.uint16)
                        )
                        dens_tiles.append(density_scaled.astype("<u2").tobytes())
                        pbar.update(1)

        del roads_df, roads_proj, roads_simple

    if dens_tiles:
        raw_data = b"".join(dens_tiles)

        dens1 = work / f"DENS{disc_code}1.SDL"
        dens1.write_bytes(raw_data)
        global_files.append(dens1)

        dens0 = work / f"DENS{disc_code}0.SDL"
        payload_dens0 = struct.pack(">I", len(dens_tiles))
        dens0_parcel = encode_bytes(
            pid=PIDS.DENS,
            payload=payload_dens0,
            compress_type=NO_COMPRESSION
        )
        dens0.write_bytes(safe_encode_parcel(PIDS.DENS, payload_dens0, 0))
        global_files.append(dens0)

    # -------------------------------------------------------------------------
    # 6. PER-REGION SDLs (NAV names, CARTO, BTREE, ROUTING)
    # -------------------------------------------------------------------------
    region_files: list[pathlib.Path] = []

    for region in regions:
        db_id = db_for_region[region]

        pbf_path = work / f"{region.replace('/', '-')}.osm.pbf"
        roads_df = load_road_network(str(pbf_path))[["id", "name", "geometry"]]

        stem = pathlib.Path(region).name.upper().replace("-", "_")

        minx, miny, maxx, maxy = roads_df.total_bounds
        min_lat_ntu, min_lon_ntu = deg_to_ntu(miny, minx)
        max_lat_ntu, max_lon_ntu = deg_to_ntu(maxy, maxx)
        region_rect_ntu = (min_lat_ntu, max_lat_ntu, min_lon_ntu, max_lon_ntu)

        # --- NAV NAME PARCELS (PID NAV) --------------------------------------
        MAX_NAV_NAME_STRINGS = 2000
        fast_file = work / f"{stem}0.SDL"
        names = roads_df["name"].fillna("").tolist()

        if not names:
            fast_file.write_bytes(safe_encode_parcel(PIDS.NAV, b'', 0))
        else:
            with open(fast_file, "wb") as f:
                for i in tqdm(range(0, len(names), MAX_NAV_NAME_STRINGS),
                              desc=f"NAV Name Parcels {stem}"):
                    name_chunk = names[i:i + MAX_NAV_NAME_STRINGS]
                    raw_payload = encode_strings(PIDS.NAV, name_chunk)
                    parcel_with_header = safe_encode_parcel(PIDS.NAV, raw_payload, 0)
                    f.write(parcel_with_header)

        # --- CARTO + BTREE chunking ------------------------------------------
        records: list[tuple[int, list[tuple[float, float]]]] = []
        for wid, geom in tqdm(zip(roads_df["id"], roads_df.geometry),
                              total=len(roads_df),
                              unit="road",
                              desc=f"Processing Roads {region}"):
            coords = list(_iter_coords(geom))
            if not coords:
                continue
            records.append((wid, coords))

        MAX_CARTO_RECORDS = 200
        record_chunks = []
        offset_chunks = []
        idx = 0
        while idx < len(records):
            chunk_records = []
            chunk_offsets = []
            off = 18  # internal offset inside carto payload

            while idx < len(records) and len(chunk_records) < MAX_CARTO_RECORDS:
                way_id, coords = records[idx]
                chunk_records.append((way_id, coords))

                size = 6 + len(coords) * 8
                chunk_offsets.append((way_id, off))
                off += size
                idx += 1

            if chunk_records:
                record_chunks.append(chunk_records)
                offset_chunks.append(chunk_offsets)

        nodes, segments = build_routing_graph_from_roads_df(roads_df)
        scale_shift = choose_scale_shift_for_nodes(nodes)

        map_file = work / f"{stem}1.SDL"
        parcel_builders: list[ParcelBuilder] = []

        # CARTO + BTREE
        for chunk_records, chunk_offsets in zip(record_chunks, offset_chunks):

            def make_carto_parcel(offset_units: int,
                                  _records=chunk_records,
                                  _rect=region_rect_ntu):
                return encode_cartography(
                    PIDS.CARTO,
                    _records,
                    offset_units=offset_units,
                    rect_ntu=_rect,
                    compress_type=NO_COMPRESSION,
                )

            def make_btree_parcel(offset_units: int,
                                  _offsets=chunk_offsets):
                return encode_btree(
                    PIDS.BTREE,
                    _offsets,
                    offset_units=offset_units,
                    compress_type=NO_COMPRESSION,
                )

            parcel_builders.append(
                ParcelBuilder(
                    pid=PIDS.CARTO,
                    layer_type=0,
                    make=make_carto_parcel,
                    rect=region_rect_ntu,
                    scale_min=0,
                    scale_max=0xFFFF,
                )
            )
            parcel_builders.append(
                ParcelBuilder(
                    pid=PIDS.BTREE,
                    layer_type=2,
                    make=make_btree_parcel,
                    rect=region_rect_ntu,
                    scale_min=0,
                    scale_max=0xFFFF,
                )
            )

        # ROUTING (chunked)
        MAX_ROUTING_CHUNK = 1000  # ~20-30KB payload per chunk
        for i in range(0, len(nodes), MAX_ROUTING_CHUNK):
            n_chunk = nodes[i:i + MAX_ROUTING_CHUNK]
            s_chunk = segments[i:i + MAX_ROUTING_CHUNK] if i < len(segments) else []

            def make_routing_parcel(offset_units: int,
                                    _nodes=n_chunk,
                                    _segments=s_chunk,
                                    _scale_shift=scale_shift,
                                    _rect=region_rect_ntu):
                return encode_routing_parcel(
                    pid=PIDS.ROUTING,
                    nodes=_nodes,
                    segments=_segments,
                    region=1,
                    parcel_type=0,
                    parcel_desc=0x02,
                    offset_units=offset_units,
                    rect_ntu=_rect,
                    scale_shift=_scale_shift,
                    size_index=0,
                    compress_type=NO_COMPRESSION,
                )

            parcel_builders.append(
                ParcelBuilder(
                    pid=PIDS.ROUTING,
                    layer_type=1,
                    make=make_routing_parcel,
                    rect=region_rect_ntu,
                    scale_min=0,
                    scale_max=0xFFFF,
                )
            )

        build_region_sdl_file(
            format_mode,
            map_file,
            db_id=db_id,
            sdl_name=map_file.name,
            parcel_builders=parcel_builders,
            topology_entries=topology_entries,
            pids=PIDS,
        )
        region_files.extend([fast_file, map_file])

    # -------------------------------------------------------------------------
    # 7. CARTOTOP.SDL + REGION.SDL / REGIONS.SDL + INIT.SDL + MTOC.SDL
    # -------------------------------------------------------------------------
    cartotop_path = work / "CARTOTOP.SDL"
    write_cartotop_sdl(cartotop_path, topology_entries, pid_cartotop=PIDS.CARTOTOP)
    global_files.append(cartotop_path)

    region_sdl = work / "REGION.SDL"
    regions_sdl = work / "REGIONS.SDL"
    mtoc_sdl = work / "MTOC.SDL"

    write_region_sdl(region_sdl, regions, supp_lang, countries)
    write_regions_sdl(regions_sdl, regions, supp_lang, countries)

    sdl_for_control = global_files + region_files + [region_sdl, regions_sdl]

    # INIT.SDL (OEM) or 0.SDL (SDAL std)
    init_filename = "0.SDL" if format_mode == "SDAL" else "INIT.SDL"
    init_path = work / init_filename

    if format_mode == "SDAL":
        write_init_sdl_standard(init_path, sdl_for_control, regions, supp_lang, pids=PIDS)
    else:
        write_oem_init_sdl(init_path, sdl_for_control, regions, supp_lang)

    all_for_mtoc = sdl_for_control + [init_path]
    write_mtoc_sdl(mtoc_sdl, all_for_mtoc)

    # -------------------------------------------------------------------------
    # 8. BUILD ISO
    # -------------------------------------------------------------------------
    iso_tmp = work / "sdal_tmp.iso"
    all_sdl_for_iso = all_for_mtoc + [mtoc_sdl]
    if iso_tmp.exists():
        iso_tmp.unlink()

    build_iso(all_sdl_for_iso, iso_tmp)
    shutil.move(str(iso_tmp), out_iso)


def cli():
    parser = argparse.ArgumentParser(description="Build SDAL ISO image from OSM data.")
    parser.add_argument("regions", nargs="+")
    parser.add_argument("--out", required=True)
    parser.add_argument("--work", required=True)
    parser.add_argument("--region-label", default=None)
    parser.add_argument("--supp-lang", default=None)
    parser.add_argument("--format-mode", default="OEM", choices=["OEM", "SDAL"])
    parser.add_argument("-v", "--verbose", action="store_true")

    args = parser.parse_args()
    init_logging(args.verbose, pathlib.Path(args.work))

    build(
        args.regions,
        pathlib.Path(args.out),
        pathlib.Path(args.work),
        region_label=args.region_label,
        supp_lang=args.supp_lang,
        format_mode=args.format_mode,
    )


if __name__ == "__main__":
    cli()
